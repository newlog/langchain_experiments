import os

from langchain.document_loaders import ReadTheDocsLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone

import pinecone

from typing import NoReturn
from langchain.schema import Document
from langchain.vectorstores.base import VectorStore


def insert_docs_vectors() -> NoReturn:
    initialize_pinecone()
    documents = load_text_data()
    print(f'Loaded {len(documents)} from LangChain documentation')
    documents = split_documents(documents)
    print(f'Split into {len(documents)} documents')
    embeddings = create_embeddings_instance()
    vector_store = insert_to_vectordb(documents, embeddings)


def initialize_pinecone() -> NoReturn:
    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment="us-west1-gcp-free")


def load_text_data() -> list[Document]:
    loader = ReadTheDocsLoader(path='./langchain_docs/python.langchain.com/en/latest')
    return loader.load()


def split_documents(documents: list[Document]) -> list[Document]:
    # Chunk size of 400 tokens. Every chunk can share a max of 50 tokens, and the chunks
    # will be separated based on the specified separators (in order of list priority)
    # https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50, separators=['\n\n', '\n', ' ', ''])
    documents = text_splitter.split_documents(documents=documents)
    for doc in documents:
        old_path = doc.metadata['source']
        new_url = old_path.replace('langchain_docs', 'https:/')
        doc.metadata.update({'source': new_url})
    return documents


def create_embeddings_instance() -> OpenAIEmbeddings:
    # By default, the embeddings model used is text-embedding-ada-002.
    # This is important because Pinecone (vector db) will ask for the dimensions (euclidean) of the
    # embeddings later on, which are found in the OpenAI's documentation for that model.
    # The embeddings will be generated by the LangChain Pinecone wrapper using from_documents() function.
    embeddings = OpenAIEmbeddings()
    return embeddings


def insert_to_vectordb(
    texts: list[Document], embeddings: OpenAIEmbeddings
) -> VectorStore:
    # This will store the text and embeddings in the remote Pinecone Vector DB.
    # Take into account that this function keeps adding more and more vectors to the index even if the texts are the same.
    # It seems like embeddings might be different even though the texts are the same. Or maybe the texts are different because
    # the splitting is not deterministic (TBD).
    vectorstore = Pinecone.from_documents(
        texts, embeddings, index_name="langchain-docs-index"
    )
    return vectorstore


if __name__ == '__main__':
    insert_docs_vectors()
